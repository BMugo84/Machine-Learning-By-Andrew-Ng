{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Lab: Gradient Descent for Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "In this lab, you will:\n",
    "\n",
    "automate the process of optimizing $ w $ and $ b $ using gradient descent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "In this lab, we will make use of:\n",
    "\n",
    "NumPy, a popular library for scientific computing\n",
    "Matplotlib, a popular library for plotting data\n",
    "plotting routines in the lab_utils.py file in the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('C:/Users/BM/ML/deeplearning.mplstyle')\n",
    "from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Let's use the same two data points as before - a house with 1000 square feet sold for $300,000 and a house with 2000 square feet sold for $500,000.\n",
    "\n",
    "|Size (1000 sqft)\t|Price (1000s of dollars) |\n",
    "|-------------------|-------------------------|\n",
    "|1               \t|300                      |      \n",
    "|2               \t|500                      |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our data set\n",
    "x_train = np.array([1.0, 2.0]) #features\n",
    "y_train = np.array([300.0, 500.0])  #target value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute_Cost\n",
    "This was developed in the last lab. We'll need it again here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function to calculate the cost\n",
    "def compute_cost(x, y, w, b):\n",
    "   \n",
    "    m = x.shape[0] \n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost = 1 / (2 * m) * cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Gradient descent summary\n",
    "So far in this course, you have developed a linear model that predicts $ f_{w,b}(x^{(i)}) $:\n",
    "$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1} $$\n",
    " \n",
    "In linear regression, you utilize input training data to fit the parameters $ w,b $ ,  by minimizing a measure of the error between our predictions $ f_{w,b}(x^{(i)}) $  and the actual data $ y^{(i)} $. The measure is called the $ cost, J(w,b) $, . In training you measure the cost over all of our training samples $ x^{(i)},y^{(i)}  $\n",
    " \n",
    "$$ J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2} $$\n",
    "\n",
    "In lecture, gradient descent was described as:\n",
    "$$ \\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*} $$\n",
    "\n",
    "where, parameters $ w,b $ ,   are updated simultaneously.\n",
    "The gradient is defined as:\n",
    "$$ \\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align} $$\n",
    "\n",
    "Here simultaniously means that you calculate the partial derivatives for all the parameters before updating any of the parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Gradient Descent\n",
    "You will implement gradient descent algorithm for one feature. You will need three functions.\n",
    "\n",
    "* compute_gradient implementing equation (4) and (5) above\n",
    "* compute_cost implementing equation (2) above (code from previous lab)\n",
    "* gradient_descent, utilizing compute_gradient and compute_cost\n",
    "Conventions:\n",
    "\n",
    "* The naming of python variables containing partial derivatives follows this pattern, $  \\frac{\\partial J(w,b)}{\\partial b} $ will be dj_db.\n",
    "* w.r.t is With Respect To, as in partial derivative of $  J(wb) $ With Respect To $ b $."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute_gradient\n",
    "compute_gradient implements (4) and (5) above and returns $ \\frac{\\partial J(w,b)}{\\partial w} , \\frac{\\partial J(w,b)}{\\partial b} $. The embedded comments describe the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        f_wb = w * x[i] + b \n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "decc6fbdd57094d84920ec442f166847203c41d7431fa4e4e739dcee8b0bc571"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
